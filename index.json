[{"content":"If you\u0026rsquo;re looking for code to get started with our method, head to Getting Started With Some Code.\nA Confound in the Data We recently published new results indicating that there is a significant confound that must be controlled for when calculating similarity between neural networks: correlated, yet distinct features within individual data points. Examples of features like this are pretty straightforward and exist even at a conceptually high level: eyes and ears, wings and cockpits, tables and chairs. Really, any set of features that co-occur in inputs frequently and are correlated with the target class. To see why features like this are a problem let’s first look at the definition of the most widely used similarity metric, Linear Centered Kernel Alignment (Kornblith et al., 2019), or Linear CKA.\nLinear Centered Kernel Alignment (CKA) CKA computes a metric of similarity between two neural networks by comparing the neuron activations of each network on provided data points, usually taken from an iid test distribution. The process is simple: pass each data point through both networks and extract the activations at the layers you want to compare and stack these activations up into two matrices (one for each network). We consider the representation of an input point to be the activations recorded in a neural network at a specific layer of interest when the data point is fed through the network. We compute similarity by mean-centering the matrices along the columns, and computing the following function:\n$$ \\begin{equation} \\text{CKA}(A, B) = \\frac{ \\lVert cov(A^T, B^T) \\rVert_F^2 }{ \\lVert cov(A^T, A^T) \\rVert_F \\lVert cov(B^T, B^T) \\rVert_F } \\end{equation} $$\nThis will provide you with a score in the range \\([0, 1]\\), with higher values indicating more similar networks. What we see in the equation above, effectively, is that CKA is computing a normalized measure of the covariance between neurons across networks. Likewise, all other existing metrics for network similarity use some form of (potentially nonlinear) feature correlation.\nLinear CKA is easily translated into a few lines of PyTorch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torch import Tensor def CKA(A: Tensor, B: Tensor): # Mean center each neuron A = A - torch.mean(A, dim=0, keepdim=True) B = B - torch.mean(B, dim=0, keepdim=True) dot_product_similarity = torch.linalg.norm(torch.matmul(A.t(), B)) ** 2 normalization_x = torch.linalg.norm(torch.matmul(A.t(), A)) normalization_y = torch.linalg.norm(torch.matmul(B.t(), B)) return dot_product_similarity / (normalization_x * normalization_y) Idealized Neurons With the idea of feature correlation in mind let\u0026rsquo;s picture two networks, each having an idealized neuron. The first network has a cat-ear detector neuron\u0026ndash;it fires when there are cat ears present in the image and does not otherwise. The other network has a neuron that is quite similar, but this one is a cat-tail detector‚ which only fires when cat tails are found. These features are distinct both visually and conceptually, but their neurons will show high correlation: images containing cat tails are very likely to contain cat ears, and conversely when cat ears are not present there are likely to be no cat tails. CKA will find these networks to be quite similar, despite their reliance on entirely different features.\nOvercoming the Confound We need a way to isolate the features in an image used by a network while randomizing or discarding all others (i.e., preserve the cat-tail in an image of a cat, while randomizing / discarding every other feature, including the cat-ears).\nA technique known as Representation Inversion can do exactly this. Representation inversion was introduced by Ilyas et al. (2019) as a way to understand the features learned by robust and non-robust networks. This method constructs model-specific datasets in which all features not used by a classifier are randomized, thus removing co-occurring features that are not utilized by the model being used to produce the inversions.\nGiven a classification dataset, we randomly choose pairs of inputs that have different labels. The first of each pair will be the seed image s and the second the target image t. Using the seed image as a starting point, we perform gradient descent to find an image that induces the same activations at the representation layer, \\(\\text{Rep}(\\cdot)\\), as the target image1. The fact that we are performing a local search is critical here, because there are many possible inverse images that match the activations. We construct this image through gradient descent in input space by optimizing the following objective:\n$$ \\begin{equation} \\text{inv} = \\min_s \\lVert \\text{Rep}(s) - \\text{Rep}(t) \\rVert_2 \\end{equation} $$\nBy sampling pairs of seed and target images that have distinct labels we eliminate features correlated with the target class that are not used by the model for classification of the target class. This is illustrated below for our two idealized cat-tail and cat-ear classifiers:\nHere, our seed image is a toucan and our target image is a cat. Representation inversion through our cat-tail network will produce an inverted image retaining the pertinent features of the target image while ignoring all other irrelevant features, resulting in a toucan with a cat tail. This happens because adding cat-ear features into our seed image will not move the representation of the seed image closer to the target as the network utilizes only cat-tails for classification of cats.\nWhen our cat tail toucan is fed through both networks, we will find that while our cat-tail neuron stays active, our cat-ear neuron never fires as this feature isn\u0026rsquo;t present! We’ve successfully isolated the features of the blue network in our target image and now can calculate similarity more accurately. The above figure also illustrates this process for the cat-ear network, however, representation inversion under this network produces a toucan with ears rather than a tail.\nBy repeating this process on many pairs of images sampled from a dataset we can produce a new inverted dataset wherein each image contains only the relevant features for the model while all others have been randomized. Calculating similarity between our inverting-model and an arbitrary network using the inverted dataset should now give us a much more accurate estimation of their similarity 2.\nResults Above we present two heatmaps showing CKA similarity calculated across pairs of 9 architectures trained on ImageNet. The left heatmap shows similarity on a set of images taken from the ImageNet validation set, as is normally done. Across the board we see that similarity is quite high between any pair of architectures, averaging at [TODO]. On the right we calculate similarity between the same set of architectures, however each row and \\(0.67\\) column pair is evaluated using the row model’s inverted dataset. Here we see that similarity is actually quite low when we isolate the features used by models!\nAlongside these results we investigated how robust training affects the similarity of architectures under our proposed metric and multiple others. Surprisingly, we found that as the robustness of an architecture increases so too does its similarity to every other architecture, at any level of robustness. If you’re interested in learning more, we invite you to give the paper a read.\nGetting Started With Some Code If you’d like to give this method a shot with your own models and datasets, we provide some code to get you started using PyTorch and the Robustness library. This code expects your model to be an AttackerModel provided by the Robustness library‚ for custom architectures check the documentation here to see how to convert your model to one, it’s not too hard.\nAll you need to do is provide the function invert_images with a model (AttackerModel), a batched set of seed images, and a batched set of target images (one for each seed image)\u0026ndash;all other hyperparameters default to the values used in our paper.\nBefore you start there are a couple things to double check:\nMake sure that your seed and target pairs are from different classes. Make sure that your models are in evaluation mode. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 from typing import Tuple import torch from robustness import attack_steps from robustness.attacker import AttackerModel from torch import Tensor from tqdm import tqdm class L2MomentumStep(attack_steps.AttackerStep): \u0026#34;\u0026#34;\u0026#34;L2 Momentum for faster convergence of inversion process\u0026#34;\u0026#34;\u0026#34; def __init__( self, orig_input: Tensor, eps: float, step_size: float, use_grad: bool = True, momentum: float = 0.9, ): super().__init__(orig_input, eps, step_size, use_grad=use_grad) self.momentum_g = torch.zeros_like(orig_input) self.gamma = momentum def project(self, x: Tensor) -\u0026gt; Tensor: \u0026#34;\u0026#34;\u0026#34;Ensures inversion does not go outside of `self.eps` L2 ball\u0026#34;\u0026#34;\u0026#34; diff = x - self.orig_input diff = diff.renorm(p=2, dim=0, maxnorm=self.eps) return torch.clamp(self.orig_input + diff, 0, 1) def step(self, x: Tensor, g: Tensor) -\u0026gt; Tensor: \u0026#34;\u0026#34;\u0026#34;Steps along gradient with L2 momentum\u0026#34;\u0026#34;\u0026#34; g = g / g.norm(dim=(1, 2, 3), p=2, keepdim=True) self.momentum_g = self.momentum_g * self.gamma + g * (1.0 - self.gamma) return x + self.momentum_g * self.step_size def inversion_loss( model: AttackerModel, inp: Tensor, targ: Tensor ) -\u0026gt; Tuple[Tensor, None]: \u0026#34;\u0026#34;\u0026#34;L2 distance between target representation and current inversion representation\u0026#34;\u0026#34;\u0026#34; _, rep = model(inp, with_latent=True, fake_relu=False) loss = torch.div(torch.norm(rep - targ, dim=1), torch.norm(targ, dim=1)) return loss, None def invert_images( model: AttackerModel, seed_images: Tensor, target_images: Tensor, batch_size: int = 32, step_size: float = 1.0 / 8.0, iterations: int = 2_000, use_best: bool = True, ) -\u0026gt; Tensor: \u0026#34;\u0026#34;\u0026#34; Representation inversion process as described in `If You\u0026#39;ve Trained One You\u0026#39;ve Trained Them All: Inter-Architecture Similarity Increases With Robustness` Default hyperparameters are exactly as used in paper. Parameters ---------- `model` : AttackerModel Model to invert through, should be a robustness.attacker.AttackerModel `seed_images` : Tensor Tensor of seed images, [B, C, H, W] `target_images` : Tensor Tensor of corresponding target images [B, C, H, W] `batch_size` : int, optional Number of images to invert at once `step_size` : float, optional \u0026#39;learning rate\u0026#39; of backprop step `iterations` : int, optional Number of back prop iterations `use_best` : bool Use best inversion found rather than last Returns ------- Tensor Resulting inverted images [B, C, H, W] \u0026#34;\u0026#34;\u0026#34; # L2 Momentum step def constraint(orig_input, eps, step_size): return L2MomentumStep(orig_input, eps, step_size) # Arguments for inversion kwargs = { \u0026#34;constraint\u0026#34;: constraint, \u0026#34;step_size\u0026#34;: step_size, \u0026#34;iterations\u0026#34;: iterations, \u0026#34;eps\u0026#34;: 1000, # Set to large number as we are not constraining inversion \u0026#34;custom_loss\u0026#34;: inversion_loss, \u0026#34;targeted\u0026#34;: True, # Minimize loss \u0026#34;use_best\u0026#34;: use_best, \u0026#34;do_tqdm\u0026#34;: False, } # Batch input seed_batches = seed_images.split(batch_size) target_batches = target_images.split(batch_size) # Begin inversion process inverted = [] for init_imgs, targ_imgs in tqdm( zip(seed_batches, target_batches), total=len(seed_batches), leave=True, desc=\u0026#34;Inverting\u0026#34;, ): # Get activations from target images (_, rep_targ), _ = model(targ_imgs.cuda(), with_latent=True) # Minimize distance from seed representation to target representation (_, _), inv = model( init_imgs.cuda(), rep_targ, make_adv=True, with_latent=True, **kwargs ) inverted.append(inv.detach().cpu()) inverted = torch.vstack(inverted) return inverted Conclusions While I mainly focused on our proposed metric in this article, I briefly wanted to discuss some of the interesting takeaways we included in the paper. The fact that robustness systematically increases a model’s similarity to any arbitrary other model, regardless of architecture or initialization, is a significant one. Within the representation learning community, some researchers have posed a universality hypothesis. This hypothesis conjectures that the features networks learn from their data are universal‚ in that they are shared across distinct initializations or architectures. Our results imply a modified universality hypothesis, suggesting that under sufficient constraints (i.e., a robustness constraint), diverse architectures will converge on a similar set of learned features. This could mean that empirical analysis of a single robust neural network can reveal insight into every other neural network\u0026ndash;possibly bringing us closer to understanding the nature of adversarial robustness itself. This is especially exciting in light of research looking at similarities between the representations learned by neural networks and brains.\nReferences [1] Haydn T. Jones, Jacob M. Springer, Garrett T. Kenyon, and Juston S. Moore. \u0026ldquo;If You\u0026rsquo;ve Trained One You\u0026rsquo;ve Trained Them All: Inter-Architecture Similarity Increases With Robustness.\u0026rdquo; UAI (2022).\n[2] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. \u0026ldquo;Similarity of Neural Network Representations Revisited.\u0026rdquo; ICML (2019).\n[3] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. “Adversarial Examples Are Not Bugs, They are Features.” NeurIPS (2019).\n[4] Vedant Nanda, Till Speicher, Camila Kolling, John P. Dickerson, Krishna Gummadi, and Adrian Weller. \u0026ldquo;Measuring Representational Robustness of Neural Networks Through Shared Invariances.” ICML (2022).\nLA-UR-22-27916\nWe define the representation layer to be the layer before the fully connected output layer. We are most interested in this layer as it should have the richest representation of the input image.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt turns out that our metric of network similarity was simultaneously proposed and published by Nanda et al. in \u0026ldquo;Measuring Representational Robustness of Neural Networks Through Shared Invariances”, where they call this metric “STIR”.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://haydn.fgl.dev/posts/a-better-index-of-similarity/","summary":"If you train one cat you get a toucan for free.","title":"A Better Metric of Neural Network Similarity"},{"content":"We\u0026rsquo;re Launching at waifuxl.com! Today we\u0026rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we\u0026rsquo;re really excited to share it with you.\nWaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different. When you use our service to upscale an image, rather than sending your input to a backend somewhere in the cloud to be up-sampled remotely, we send the up-sampling neural network (and the tagger) to you for execution directly on your laptop, desktop, phone, or tablet. We\u0026rsquo;ll get to how this is possible in a moment, but first we\u0026rsquo;re going to cover the models.\nThe Networks Super Resolution What sets the Real-ESRGAN apart from other models (and the models used by waifu2x) is not the architecture but it’s training process. The standard training process for super resolution networks is to simply take a high-resolution dataset of images, downscale them to a lower resolution, and train the network to map from the downscaled images to the original high-resolution ones. The Real-ESRGAN training process attempts to directly model the kind of degradations one might encounter in real-world low-quality images through a process they call a \u0026ldquo;high-order degradation model\u0026rdquo;. During this process they combine many different degradations with various intensities to produce a visually corrupt input to the network. Here is an overview of the various degradations they apply, but refer to Section 3 of the paper to get a better idea of the entire process:\nBlur (multiple kinds) Noise Combinations of up-sampling and down-sampling with various algorithms JPEG compression Sinc filters (to model ringing and overshoot) The training process is further improved by including a GAN loss (wherein a separate network learns to differentiate between the true high-res images and the super resolution network outputs, causing the two networks to compete) and a Perceptual loss.\nImage Tagging For our image tagger we\u0026rsquo;re using a MobileNetV3. The network will detect 2,000 different characteristics in the image, 2,000 different characters (mostly from anime or related media), and an explicitness rating (safe, questionable, or explicit). Here is an example of our tagger outputs:\nCharacteristics Number of girls / boys in the image Eye color Hair color Clothing types Characters: Naruto Hatsune Miku Megumin The Dataset To train both of our models we are using the Danbooru2021 dataset, comprised of ~4.9m images scraped from the Danbooru image board. Due to storage limitations we trained both models on a subset of ~1.2m images, however, this is likely a sizable enough subset to not have caused any problems. The tags in the Danbooru2021 dataset are quite diverse, with over 493,000 distinct tags, but they are extremely unbalanced. The most common tag is \u0026ldquo;1girl\u0026rdquo; which is present on millions of the images and by the time you get down to the 1,000th most common tag it is only found on ~10,000 images. The unbalanced nature of the tags certainly has resulted in a less than optimal tagger, but there is little that can be done about that.\nThe Website The Not-Backend Previously we mentioned that rather than running the models on a backend in the cloud, we\u0026rsquo;re serving you the models for execution on your machine\u0026ndash;how is this done? The Onnx Runtime is an open source project aiming to accelerate machine learning training across a wide variety of frameworks, devices, and deployment targets\u0026ndash;one of these targets being the web through a WebAssembly backend. After exporting the model to the onnx format we simply serve the model to you and their backend will take care of the rest. Best of all it supports SIMD acceleration and multithreading.\nThe Upscaling Lifecycle Generally, the lifecycle of WaifuXL can be described as taking in an input image, loading in the model, running the model on the image, and then creating an output image from the model results. There are more granular steps in this process that provide various tools and functionality, such as image chunking and GIF de/restructuring.\nIn order to efficiently pass data between different services and UI components, all of the images are represented throughout the lifecycle as Data URIs. If you\u0026rsquo;re not familiar, data URIs are a long string that basically provides a text encoding of a file, so they allow for you to store the image purely as a string. They begin with a small section describing the file format (data:image/gif or data:image/png), and then a long string that encodes the actual file contents.\nWhen the actual pixel information is displayed to a user on the site, these data URIs are simply passed into JSX image tags and displayed. When the various services need to actually gather information from the images (such as for chunking, or to build an ndarray to pass to the model) the data URIs are painted on an HTML canvas (this canvas is never shown to the user) and the getImageData function is called on the canvas context.\nThe actual function we use to do this looks as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function getPixelDataFromURI(inputURI) { return new Promise((resolve, reject) =\u0026gt; { const img = new Image(); img.src = inputURI; img.crossOrigin = \u0026#34;Anonymous\u0026#34;; var results = null; img.onload = function () { const canvas = document.createElement(\u0026#34;canvas\u0026#34;); canvas.width = img.width; canvas.height = img.height; const context = canvas.getContext(\u0026#34;2d\u0026#34;); context.drawImage(img, 0, 0); resolve(context.getImageData(0, 0, img.width, img.height)); }; }); } After we have all the pixel information in the image, we then convert this to an ndarray, using the ndarray and ndarray-ops libraries.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 export function buildNdarrayFromImage(imageData) { const { data, width, height } = imageData; const dataTensor = ndarray(new Uint8Array(data), [height, width, 4]); const dataProcessedTensor = ndarray(new Uint8Array(width * height * 3), [ 1, 3, height, width, ]); ops.assign( dataProcessedTensor.pick(0, 0, null, null), dataTensor.pick(null, null, 0) ); ops.assign( dataProcessedTensor.pick(0, 1, null, null), dataTensor.pick(null, null, 1) ); ops.assign( dataProcessedTensor.pick(0, 2, null, null), dataTensor.pick(null, null, 2) ); return dataProcessedTensor; } Finally, this ndarray is converted to a Tensor for the model input, as follows:\n1 2 3 4 5 6 7 8 9 10 11 function prepareImage(imageArray) { const height = imageArray.shape[2]; const width = imageArray.shape[3]; const tensor = new ort.Tensor(\u0026#34;uint8\u0026#34;, imageArray.data, [ 1, 3, height, width, ]); return { input: tensor }; } The model then runs, using this new Tensor as input, and produces a new Tensor. This Tensor is converted to an ndarray, and we paint this ndarray to a canvas once again, and return the Data URI from this canvas.\n1 2 3 4 5 6 7 8 9 10 export function buildImageFromND(nd, height, width) { const canvas = document.createElement(\u0026#34;canvas\u0026#34;); canvas.width = width; canvas.height = height; const context = canvas.getContext(\u0026#34;2d\u0026#34;); var data = context.createImageData(width, height); data.data.set(nd); context.putImageData(data, 0, 0); return canvas.toDataURL(); } Finally, now that we have the output Data URI, we can render this output alongside the input to the user, using the react-compare-slider library to create a side-by-side view of the images.\nBreakdown of Some \u0026ldquo;Fun\u0026rdquo; Problems Image Splitting After some initial testing with upscaling large images, we realized that with a sufficiently large image, the upscaling model would fail. The cause of this turned out to be the 32-bit address space of WebAssembly, meaning our address space, and therefore our memory usage, is fundamentally limited. To get around this problem, we split large images into chunks and process the individual chunks instead of processing the entire image at once.\nThat would\u0026rsquo;ve been the end of the story if it weren\u0026rsquo;t for the windowing artifacts. When image chunks are upscaled, there are visible artifacts along their boundaries in the combined image, shown below:\nTo get around this we allow chunks to overlap onto other chunks when they are upscaled, and then remove/merge the overlapping regions. We found that an overlap of only 4 pixels to each side was sufficient to remove the windowing effect.\nGIFs GIFs work almost the exact same way that images do, except for on input they are split into each frame, each of these frames is upscaled individually, and then the frames are reassembled into a new GIF. This is basically just running the image upscaling process on each frame, and then combining these frames into a new GIF. This is accomplished with the help of gif-js, gifuct-js, and gif-extract-frames.\nThe most challenging parts of this process was handling coalescing, since GIFs often only store information about pixels changed between frames rather than each frame of the GIF. Generally, gif-extract-frames is used to break the image down into individual frames, gif.js is used to create new GIFs, and gifuct-js primarily handles extracting the delay from GIFs.\nSafari Non-Compliance One of the more interesting problems arose from a classic Web Development hurdle: non-compliant browsers. And of course, as Internet Explorer has been sunset, a new champion of non-compliance rises \u0026ndash; Apple\u0026rsquo;s Safari browser. While significantly better than previously non-compliant browsers, Safari still offers various \u0026ldquo;fun\u0026rdquo; things it\u0026rsquo;s decided not to implement. In this project, our main issues came from (what we believe) is Safari\u0026rsquo;s SharedArrayBuffer implementation \u0026ndash; namely that it doesn\u0026rsquo;t seem to work. The ONNX web runtime uses SharedArrayBuffers when running multi-threaded, so in Safari, trying to initialize the model to use more than one thread fails. At this time, we\u0026rsquo;re getting around this by checking the User Agent of the browser, and if its a Webkit based browser / engine we fall back to serial execution. We\u0026rsquo;ve submitted an issue with ONNX to resolve this, and hopefully we will be able to give Webkit based browser users better speeds in the future.\nAs a sidenote, to enable SharedArrayBuffers in general you must set two response headers\u0026ndash;Cross Origin Embedder Policy and Cross Origin Opener Policy. When you don\u0026rsquo;t set these headers properly, there will be no issue thrown on any browser, as it is impossible for SharedArrayBuffers to be used at all. This led to plenty of confusion in local testing and early trials, as it became difficult to efficiently test changes and debug the issue locally.\nDisagreement between PyTorch and ONNX One operation used by the Real-ESRGAN is the Pixel UnShuffle, an operation where spatial width and height of an image (or Tensor) are traded for depth by squeezing some of the spatial information into the channel dimension. Both PyTorch and ONNX support this operation, however, they were performing this squeezing in different orders. This was resulting in an upscaled image that looks like the colors were inverted\u0026ndash;not great. An issue was opened in PyTorch and in the meantime we implemented the operator from scratch. About a week ago the issue was finally resolved, and we were able to use the built in version.\nWrap-up and Future Work for the Future Gadgets Lab It was a bit all over the place and rushed, but we hope you enjoyed our write up on WaifuXL. This was our first big project as a group, so we were excited to share some details on our effort.\nWe hope you like WaifuXL, we\u0026rsquo;re really happy with the quality of the model (of course, props to the researchers behind the Real-ESRGAN) and our method of delivery. Our model only performs well on anime-style drawings but we\u0026rsquo;d like to train a model on real images so we can provide high quality up-sampling for all types of images. We\u0026rsquo;re also interested in adding some more models to the website, such as a style transfer model, but we\u0026rsquo;re likely going to leave that to a later date.\nStay tuned for some more fun projects from us, we\u0026rsquo;re always throwing around ideas and maybe we\u0026rsquo;ll land on another one like this soon. Until then, keep expanding your Waifus.\n","permalink":"https://haydn.fgl.dev/posts/the-launch-of-waifuxl/","summary":"We\u0026rsquo;re Launching at waifuxl.com! Today we\u0026rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we\u0026rsquo;re really excited to share it with you.\nWaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different.","title":"The Launch of WaifuXL"}]