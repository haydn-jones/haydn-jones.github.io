<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Better Metric of Neural Network Similarity | Posterior Collapse</title><meta name=keywords content="ML"><meta name=description content="If you train one cat you get a toucan for free."><meta name=author content="Haydn Jones"><link rel=canonical href=https://haydn.fgl.dev/posts/a-better-index-of-similarity/><link crossorigin=anonymous href=/assets/css/stylesheet.153d14da20415c74febc27298623ff1f5da86583c5f8526cd1dca9fb2a2adf11.css integrity="sha256-FT0U2iBBXHT+vCcphiP/H12oZYPF+FJs0dyp+yoq3xE=" rel="preload stylesheet" as=style><link rel=icon href=https://haydn.fgl.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://haydn.fgl.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://haydn.fgl.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://haydn.fgl.dev/apple-touch-icon.png><link rel=mask-icon href=https://haydn.fgl.dev/favicon-32x32.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta name=twitter:card content="summary"><meta name=twitter:title content="A Better Metric of Neural Network Similarity"><meta name=twitter:description content="If you train one cat you get a toucan for free."><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V58YRV80SX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-V58YRV80SX",{anonymize_ip:!1})}</script><meta property="og:title" content="A Better Metric of Neural Network Similarity"><meta property="og:description" content="If you train one cat you get a toucan for free."><meta property="og:type" content="article"><meta property="og:url" content="https://haydn.fgl.dev/posts/a-better-index-of-similarity/"><meta property="og:image" content="https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram.webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-01T11:50:15-06:00"><meta property="article:modified_time" content="2022-08-03T21:17:29+02:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram.webp"><meta name=twitter:title content="A Better Metric of Neural Network Similarity"><meta name=twitter:description content="If you train one cat you get a toucan for free."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://haydn.fgl.dev/posts/"},{"@type":"ListItem","position":2,"name":"A Better Metric of Neural Network Similarity","item":"https://haydn.fgl.dev/posts/a-better-index-of-similarity/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Better Metric of Neural Network Similarity","name":"A Better Metric of Neural Network Similarity","description":"If you train one cat you get a toucan for free.","keywords":["ML"],"articleBody":"If you’re looking for code to get started with our method, head to Getting Started With Some Code.\nA Confound in the Data We recently published new results indicating that there is a significant confound that must be controlled for when calculating similarity between neural networks: correlated, yet distinct features within individual data points. Examples of features like this are pretty straightforward and exist even at a conceptually high level: eyes and ears, wings and cockpits, tables and chairs. Really, any set of features that co-occur in inputs frequently and are correlated with the target class. To see why features like this are a problem let’s first look at the definition of the most widely used similarity metric, Linear Centered Kernel Alignment (Kornblith et al., 2019), or Linear CKA.\nLinear Centered Kernel Alignment (CKA) CKA computes a metric of similarity between two neural networks by comparing the neuron activations of each network on provided data points, usually taken from an iid test distribution. The process is simple: pass each data point through both networks and extract the activations at the layers you want to compare and stack these activations up into two matrices (one for each network). We consider the representation of an input point to be the activations recorded in a neural network at a specific layer of interest when the data point is fed through the network. We compute similarity by mean-centering the matrices along the columns, and computing the following function:\n$$ \\begin{equation} \\text{CKA}(A, B) = \\frac{ \\lVert cov(A^T, B^T) \\rVert_F^2 }{ \\lVert cov(A^T, A^T) \\rVert_F \\lVert cov(B^T, B^T) \\rVert_F } \\end{equation} $$\nThis will provide you with a score in the range \\([0, 1]\\), with higher values indicating more similar networks. What we see in the equation above, effectively, is that CKA is computing a normalized measure of the covariance between neurons across networks. Likewise, all other existing metrics for network similarity use some form of (potentially nonlinear) feature correlation.\nLinear CKA is easily translated into a few lines of PyTorch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torch import Tensor def CKA(A: Tensor, B: Tensor): # Mean center each neuron A = A - torch.mean(A, dim=0, keepdim=True) B = B - torch.mean(B, dim=0, keepdim=True) dot_product_similarity = torch.linalg.norm(torch.matmul(A.t(), B)) ** 2 normalization_x = torch.linalg.norm(torch.matmul(A.t(), A)) normalization_y = torch.linalg.norm(torch.matmul(B.t(), B)) return dot_product_similarity / (normalization_x * normalization_y) Idealized Neurons With the idea of feature correlation in mind let’s picture two networks, each having an idealized neuron. The first network has a cat-ear detector neuron–it fires when there are cat ears present in the image and does not otherwise. The other network has a neuron that is quite similar, but this one is a cat-tail detector‚ which only fires when cat tails are found. These features are distinct both visually and conceptually, but their neurons will show high correlation: images containing cat tails are very likely to contain cat ears, and conversely when cat ears are not present there are likely to be no cat tails. CKA will find these networks to be quite similar, despite their reliance on entirely different features.\nOvercoming the Confound We need a way to isolate the features in an image used by a network while randomizing or discarding all others (i.e., preserve the cat-tail in an image of a cat, while randomizing / discarding every other feature, including the cat-ears).\nA technique known as Representation Inversion can do exactly this. Representation inversion was introduced by Ilyas et al. (2019) as a way to understand the features learned by robust and non-robust networks. This method constructs model-specific datasets in which all features not used by a classifier are randomized, thus removing co-occurring features that are not utilized by the model being used to produce the inversions.\nGiven a classification dataset, we randomly choose pairs of inputs that have different labels. The first of each pair will be the seed image s and the second the target image t. Using the seed image as a starting point, we perform gradient descent to find an image that induces the same activations at the representation layer, \\(\\text{Rep}(\\cdot)\\), as the target image1. The fact that we are performing a local search is critical here, because there are many possible inverse images that match the activations. We construct this image through gradient descent in input space by optimizing the following objective:\n$$ \\begin{equation} \\text{inv} = \\min_s \\lVert \\text{Rep}(s) - \\text{Rep}(t) \\rVert_2 \\end{equation} $$\nBy sampling pairs of seed and target images that have distinct labels we eliminate features correlated with the target class that are not used by the model for classification of the target class. This is illustrated below for our two idealized cat-tail and cat-ear classifiers:\nHere, our seed image is a toucan and our target image is a cat. Representation inversion through our cat-tail network will produce an inverted image retaining the pertinent features of the target image while ignoring all other irrelevant features, resulting in a toucan with a cat tail. This happens because adding cat-ear features into our seed image will not move the representation of the seed image closer to the target as the network utilizes only cat-tails for classification of cats.\nWhen our cat tail toucan is fed through both networks, we will find that while our cat-tail neuron stays active, our cat-ear neuron never fires as this feature isn’t present! We’ve successfully isolated the features of the blue network in our target image and now can calculate similarity more accurately. The above figure also illustrates this process for the cat-ear network, however, representation inversion under this network produces a toucan with ears rather than a tail.\nBy repeating this process on many pairs of images sampled from a dataset we can produce a new inverted dataset wherein each image contains only the relevant features for the model while all others have been randomized. Calculating similarity between our inverting-model and an arbitrary network using the inverted dataset should now give us a much more accurate estimation of their similarity 2.\nResults Above we present two heatmaps showing CKA similarity calculated across pairs of 9 architectures trained on ImageNet. The left heatmap shows similarity on a set of images taken from the ImageNet validation set, as is normally done. Across the board we see that similarity is quite high between any pair of architectures, averaging at [TODO]. On the right we calculate similarity between the same set of architectures, however each row and \\(0.67\\) column pair is evaluated using the row model’s inverted dataset. Here we see that similarity is actually quite low when we isolate the features used by models!\nAlongside these results we investigated how robust training affects the similarity of architectures under our proposed metric and multiple others. Surprisingly, we found that as the robustness of an architecture increases so too does its similarity to every other architecture, at any level of robustness. If you’re interested in learning more, we invite you to give the paper a read.\nGetting Started With Some Code If you’d like to give this method a shot with your own models and datasets, we provide some code to get you started using PyTorch and the Robustness library. This code expects your model to be an AttackerModel provided by the Robustness library‚ for custom architectures check the documentation here to see how to convert your model to one, it’s not too hard.\nAll you need to do is provide the function invert_images with a model (AttackerModel), a batched set of seed images, and a batched set of target images (one for each seed image)–all other hyperparameters default to the values used in our paper.\nBefore you start there are a couple things to double check:\nMake sure that your seed and target pairs are from different classes. Make sure that your models are in evaluation mode. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 from typing import Tuple import torch from robustness import attack_steps from robustness.attacker import AttackerModel from torch import Tensor from tqdm import tqdm class L2MomentumStep(attack_steps.AttackerStep): \"\"\"L2 Momentum for faster convergence of inversion process\"\"\" def __init__( self, orig_input: Tensor, eps: float, step_size: float, use_grad: bool = True, momentum: float = 0.9, ): super().__init__(orig_input, eps, step_size, use_grad=use_grad) self.momentum_g = torch.zeros_like(orig_input) self.gamma = momentum def project(self, x: Tensor) -\u003e Tensor: \"\"\"Ensures inversion does not go outside of `self.eps` L2 ball\"\"\" diff = x - self.orig_input diff = diff.renorm(p=2, dim=0, maxnorm=self.eps) return torch.clamp(self.orig_input + diff, 0, 1) def step(self, x: Tensor, g: Tensor) -\u003e Tensor: \"\"\"Steps along gradient with L2 momentum\"\"\" g = g / g.norm(dim=(1, 2, 3), p=2, keepdim=True) self.momentum_g = self.momentum_g * self.gamma + g * (1.0 - self.gamma) return x + self.momentum_g * self.step_size def inversion_loss( model: AttackerModel, inp: Tensor, targ: Tensor ) -\u003e Tuple[Tensor, None]: \"\"\"L2 distance between target representation and current inversion representation\"\"\" _, rep = model(inp, with_latent=True, fake_relu=False) loss = torch.div(torch.norm(rep - targ, dim=1), torch.norm(targ, dim=1)) return loss, None def invert_images( model: AttackerModel, seed_images: Tensor, target_images: Tensor, batch_size: int = 32, step_size: float = 1.0 / 8.0, iterations: int = 2_000, use_best: bool = True, ) -\u003e Tensor: \"\"\" Representation inversion process as described in `If You've Trained One You've Trained Them All: Inter-Architecture Similarity Increases With Robustness` Default hyperparameters are exactly as used in paper. Parameters ---------- `model` : AttackerModel Model to invert through, should be a robustness.attacker.AttackerModel `seed_images` : Tensor Tensor of seed images, [B, C, H, W] `target_images` : Tensor Tensor of corresponding target images [B, C, H, W] `batch_size` : int, optional Number of images to invert at once `step_size` : float, optional 'learning rate' of backprop step `iterations` : int, optional Number of back prop iterations `use_best` : bool Use best inversion found rather than last Returns ------- Tensor Resulting inverted images [B, C, H, W] \"\"\" # L2 Momentum step def constraint(orig_input, eps, step_size): return L2MomentumStep(orig_input, eps, step_size) # Arguments for inversion kwargs = { \"constraint\": constraint, \"step_size\": step_size, \"iterations\": iterations, \"eps\": 1000, # Set to large number as we are not constraining inversion \"custom_loss\": inversion_loss, \"targeted\": True, # Minimize loss \"use_best\": use_best, \"do_tqdm\": False, } # Batch input seed_batches = seed_images.split(batch_size) target_batches = target_images.split(batch_size) # Begin inversion process inverted = [] for init_imgs, targ_imgs in tqdm( zip(seed_batches, target_batches), total=len(seed_batches), leave=True, desc=\"Inverting\", ): # Get activations from target images (_, rep_targ), _ = model(targ_imgs.cuda(), with_latent=True) # Minimize distance from seed representation to target representation (_, _), inv = model( init_imgs.cuda(), rep_targ, make_adv=True, with_latent=True, **kwargs ) inverted.append(inv.detach().cpu()) inverted = torch.vstack(inverted) return inverted Conclusions While I mainly focused on our proposed metric in this article, I briefly wanted to discuss some of the interesting takeaways we included in the paper. The fact that robustness systematically increases a model’s similarity to any arbitrary other model, regardless of architecture or initialization, is a significant one. Within the representation learning community, some researchers have posed a universality hypothesis. This hypothesis conjectures that the features networks learn from their data are universal‚ in that they are shared across distinct initializations or architectures. Our results imply a modified universality hypothesis, suggesting that under sufficient constraints (i.e., a robustness constraint), diverse architectures will converge on a similar set of learned features. This could mean that empirical analysis of a single robust neural network can reveal insight into every other neural network–possibly bringing us closer to understanding the nature of adversarial robustness itself. This is especially exciting in light of research looking at similarities between the representations learned by neural networks and brains.\nReferences [1] Haydn T. Jones, Jacob M. Springer, Garrett T. Kenyon, and Juston S. Moore. “If You’ve Trained One You’ve Trained Them All: Inter-Architecture Similarity Increases With Robustness.” UAI (2022).\n[2] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. “Similarity of Neural Network Representations Revisited.” ICML (2019).\n[3] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. “Adversarial Examples Are Not Bugs, They are Features.” NeurIPS (2019).\n[4] Vedant Nanda, Till Speicher, Camila Kolling, John P. Dickerson, Krishna Gummadi, and Adrian Weller. “Measuring Representational Robustness of Neural Networks Through Shared Invariances.” ICML (2022).\nLA-UR-22-27916\nWe define the representation layer to be the layer before the fully connected output layer. We are most interested in this layer as it should have the richest representation of the input image. ↩︎\nIt turns out that our metric of network similarity was simultaneously proposed and published by Nanda et al. in “Measuring Representational Robustness of Neural Networks Through Shared Invariances”, where they call this metric “STIR”. ↩︎\n","wordCount":"2193","inLanguage":"en","image":"https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram.webp","datePublished":"2022-06-01T11:50:15-06:00","dateModified":"2022-08-03T21:17:29+02:00","author":{"@type":"Person","name":"Haydn Jones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://haydn.fgl.dev/posts/a-better-index-of-similarity/"},"publisher":{"@type":"Organization","name":"Posterior Collapse","logo":{"@type":"ImageObject","url":"https://haydn.fgl.dev/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://haydn.fgl.dev/ accesskey=h title="Posterior Collapse (Alt + H)"><img src=https://haydn.fgl.dev/logo.svg alt aria-label=logo height=40>Posterior Collapse</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://haydn.fgl.dev/archives title=Archive><span>Archive</span></a></li><li><a href=https://haydn.fgl.dev/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://haydn.fgl.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://haydn.fgl.dev/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://waifuxl.com/ title=WaifuXL.com><span>WaifuXL.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://haydn.fgl.dev/>Home</a>&nbsp;»&nbsp;<a href=https://haydn.fgl.dev/posts/>Posts</a></div><h1 class=post-title>A Better Metric of Neural Network Similarity</h1><div class=post-meta><span title='2022-06-01 11:50:15 -0600 -0600'>June 1, 2022</span>&nbsp;·&nbsp;2193 words&nbsp;·&nbsp;Haydn Jones&nbsp;|&nbsp;<a href=https://github.com/haydn-jones/haydn-jones.github.io/tree/main/content//posts/A-Better-Index-of-Similarity/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img srcset="https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram_hu9d74877740a9fed251f2d5563166613b_149884_360x0_resize_q75_h2_box_2.webp 360w ,https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram_hu9d74877740a9fed251f2d5563166613b_149884_480x0_resize_q75_h2_box_2.webp 480w ,https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram_hu9d74877740a9fed251f2d5563166613b_149884_720x0_resize_q75_h2_box_2.webp 720w ,https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram_hu9d74877740a9fed251f2d5563166613b_149884_1080x0_resize_q75_h2_box_2.webp 1080w ,https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram_hu9d74877740a9fed251f2d5563166613b_149884_1500x0_resize_q75_h2_box_2.webp 1500w ,https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram.webp 2048w" sizes="(min-width: 768px) 720px, 100vw" src=https://haydn.fgl.dev/posts/a-better-index-of-similarity/images/inv_diagram.webp alt="Image inverion process" width=2048 height=1566></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#a-confound-in-the-data aria-label="A Confound in the Data">A Confound in the Data</a><ul><li><a href=#linear-centered-kernel-alignment-cka aria-label="Linear Centered Kernel Alignment (CKA)">Linear Centered Kernel Alignment (CKA)</a></li><li><a href=#idealized-neurons aria-label="Idealized Neurons">Idealized Neurons</a></li><li><a href=#overcoming-the-confound aria-label="Overcoming the Confound">Overcoming the Confound</a></li></ul></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#getting-started-with-some-code aria-label="Getting Started With Some Code">Getting Started With Some Code</a></li><li><a href=#conclusions aria-label=Conclusions>Conclusions</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>If you&rsquo;re looking for code to get started with our method, head to <a href=#getting-started-with-some-code>Getting Started With Some Code</a>.</p><h1 id=a-confound-in-the-data>A Confound in the Data<a hidden class=anchor aria-hidden=true href=#a-confound-in-the-data>#</a></h1><p><a href="https://openreview.net/pdf?id=BGfLS_8j5eq">We recently published new results</a> indicating that there is a significant confound that must be controlled for when calculating similarity between neural networks: <strong>correlated, yet distinct features <em>within</em> individual data points</strong>. Examples of features like this are pretty straightforward and exist even at a conceptually high level: eyes and ears, wings and cockpits, tables and chairs. Really, any set of features that co-occur in inputs frequently and are correlated with the target class. To see why features like this are a problem let’s first look at the definition of the most widely used similarity metric, <a href=https://arxiv.org/abs/1905.00414><strong>Linear Centered Kernel Alignment (Kornblith et al., 2019)</strong></a>, or Linear CKA.</p><h2 id=linear-centered-kernel-alignment-cka>Linear Centered Kernel Alignment (CKA)<a hidden class=anchor aria-hidden=true href=#linear-centered-kernel-alignment-cka>#</a></h2><p>CKA computes a metric of similarity between two neural networks by comparing the neuron activations of each network on provided data points, usually taken from an iid test distribution. The process is simple: pass each data point through both networks and extract the activations at the layers you want to compare and stack these activations up into two matrices (one for each network). We consider the <em>representation</em> of an input point to be the <em>activations</em> recorded in a neural network at a specific layer of interest when the data point is fed through the network. We compute similarity by mean-centering the matrices along the columns, and computing the following function:</p><p>$$
\begin{equation}
\text{CKA}(A, B) = \frac{
\lVert cov(A^T, B^T) \rVert_F^2
}{
\lVert cov(A^T, A^T) \rVert_F
\lVert cov(B^T, B^T) \rVert_F
}
\end{equation}
$$</p><p>This will provide you with a score in the range \([0, 1]\), with higher values indicating more similar networks. What we see in the equation above, effectively, is that CKA is computing a normalized measure of the covariance between neurons across networks. Likewise, all other existing metrics for network similarity use some form of (potentially nonlinear) feature correlation.</p><p>Linear CKA is easily translated into a few lines of PyTorch:</p><div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">14
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#d75f00>import</span> torch
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> torch <span style=color:#d75f00>import</span> Tensor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>CKA</span>(A: Tensor, B: Tensor):
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Mean center each neuron</span>
</span></span><span style=display:flex><span>    A = A - torch.mean(A, dim=<span style=color:#00afaf>0</span>, keepdim=<span style=color:#d75f00>True</span>)
</span></span><span style=display:flex><span>    B = B - torch.mean(B, dim=<span style=color:#00afaf>0</span>, keepdim=<span style=color:#d75f00>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dot_product_similarity = torch.linalg.norm(torch.matmul(A.t(), B)) ** <span style=color:#00afaf>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    normalization_x = torch.linalg.norm(torch.matmul(A.t(), A))
</span></span><span style=display:flex><span>    normalization_y = torch.linalg.norm(torch.matmul(B.t(), B))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> dot_product_similarity / (normalization_x * normalization_y)</span></span></code></pre></td></tr></table></div></div><h2 id=idealized-neurons>Idealized Neurons<a hidden class=anchor aria-hidden=true href=#idealized-neurons>#</a></h2><p>With the idea of feature correlation in mind let&rsquo;s picture two networks, each having an idealized neuron. The first network has a
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ear</span></code> detector neuron&ndash;it fires when there are cat ears present in the image and does not otherwise. The other network has a neuron that is quite similar, but this one is a
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tail</span></code> detector‚ which only fires when cat tails are found. These features are distinct both visually and conceptually, but their neurons will show high correlation: images containing cat tails are very likely to contain cat ears, and conversely when cat ears are not present there are likely to be no cat tails. <strong>CKA will find these networks to be quite similar, despite their reliance on entirely different features.</strong></p><h2 id=overcoming-the-confound>Overcoming the Confound<a hidden class=anchor aria-hidden=true href=#overcoming-the-confound>#</a></h2><p>We need a way to isolate the features in an image used by a network while randomizing or discarding all others (i.e., preserve the
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tail</span></code> in an image of a cat, while randomizing / discarding every other feature, including the
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ears</span></code>).</p><p>A technique known as <strong>Representation Inversion</strong> can do exactly this. Representation inversion was introduced by <a href=https://arxiv.org/abs/1905.02175><strong>Ilyas et al. (2019)</strong></a> as a way to understand the features learned by robust and non-robust networks. This method constructs model-specific datasets in which all features not used by a classifier are randomized, thus removing co-occurring features that are not utilized by the model being used to produce the inversions.</p><p>Given a classification dataset, we randomly choose pairs of inputs that have <em>different</em> labels. The first of each pair will be the <code>seed</code> image <code>s</code> and the second the <code>target</code> image <code>t</code>. Using the seed image as a starting point, we perform gradient descent to find an image that induces the same activations at the representation layer, \(\text{Rep}(\cdot)\), as the target image<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. The fact that we are performing a local search is critical here, because there are many possible inverse images that match the activations. We construct this image through gradient descent in input space by optimizing the following objective:</p><p>$$
\begin{equation}
\text{inv} = \min_s \lVert \text{Rep}(s) - \text{Rep}(t) \rVert_2
\end{equation}
$$</p><p>By sampling pairs of <code>seed</code> and <code>target</code> images that have distinct labels we eliminate features correlated with the target class that are not used by the model for classification of the target class. This is illustrated below for our two idealized
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tail</span></code> and
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ear</span></code> classifiers:</p><figure><img loading=lazy src=images/inv_diagram.webp#center alt="Image inverion process"></figure><p>Here, our <code>seed</code> image is a toucan and our <code>target</code> image is a cat. Representation inversion through our
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tail</span></code> network will produce an inverted image retaining the pertinent features of the <code>target</code> image while ignoring all other irrelevant features, resulting in a toucan with a cat tail. This happens because adding
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ear</span></code> features into our <code>seed</code> image will not move the representation of the <code>seed</code> image closer to the <code>target</code> as the network utilizes only
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tails</span></code> for classification of cats.</p><p>When our cat tail toucan is fed through both networks, we will find that while our
<code style=background-color:#346db5><span style=color:#346db5;filter:grayscale(1)invert(1)contrast(100)>cat-tail</span></code> neuron stays active, our
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ear</span></code> neuron never fires as this feature isn&rsquo;t present! We’ve successfully isolated the features of the blue network in our target image and now can calculate similarity more accurately. The above figure also illustrates this process for the
<code style=background-color:#f6b819><span style=color:#f6b819;filter:grayscale(1)invert(1)contrast(100)>cat-ear</span></code> network, however, representation inversion under this network produces a toucan with ears rather than a tail.</p><p>By repeating this process on many pairs of images sampled from a dataset we can produce a new inverted dataset wherein each image contains only the relevant features for the model while all others have been randomized. <strong>Calculating similarity between our inverting-model and an arbitrary network using the inverted dataset should now give us a much more accurate estimation of their similarity</strong> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><h1 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h1><figure><img loading=lazy src=images/results.webp#center alt="Image inverion process"></figure><p>Above we present two heatmaps showing CKA similarity calculated across pairs of 9 architectures trained on ImageNet. The left heatmap shows similarity on a set of images taken from the ImageNet validation set, as is normally done. Across the board we see that similarity is quite high between any pair of architectures, averaging at [TODO]. On the right we calculate similarity between the same set of architectures, however each row and \(0.67\) column pair is evaluated using the <strong>row model’s</strong> inverted dataset. Here we see that similarity is actually quite low when we isolate the features used by models!</p><p>Alongside these results we investigated how robust training affects the similarity of architectures under our proposed metric and multiple others. <strong>Surprisingly, we found that as the robustness of an architecture increases so too does its similarity to every other architecture, at any level of robustness.</strong> If you’re interested in learning more, we invite you to give the paper a <a href="https://openreview.net/pdf?id=BGfLS_8j5eq">read</a>.</p><h1 id=getting-started-with-some-code>Getting Started With Some Code<a hidden class=anchor aria-hidden=true href=#getting-started-with-some-code>#</a></h1><p>If you’d like to give this method a shot with your own models and datasets, we provide some code to get you started using <a href=https://pytorch.org/>PyTorch</a> and the <a href=https://robustness.readthedocs.io/>Robustness</a> library. This code expects your model to be an AttackerModel provided by the Robustness library‚ for custom architectures check the documentation <a href=https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html#training-with-custom-architectures>here</a> to see how to convert your model to one, it’s not too hard.</p><p>All you need to do is provide the function <code>invert_images</code> with a model (AttackerModel), a batched set of seed images, and a batched set of target images (one for each seed image)&ndash;all other hyperparameters default to the values used in our paper.</p><p>Before you start there are a couple things to double check:</p><ul><li>Make sure that your seed and target pairs are from different classes.</li><li>Make sure that your models are in evaluation mode.</li></ul><div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 14
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 15
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 16
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 17
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 18
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 19
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 20
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 21
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 22
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 23
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 24
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 25
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 26
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 27
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 28
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 29
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 30
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 31
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 32
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 33
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 34
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 35
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 36
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 37
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 38
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 39
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 40
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 41
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 42
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 43
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 44
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 45
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 46
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 47
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 48
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 49
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 50
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 51
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 52
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 53
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 54
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 55
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 56
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 57
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 58
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 59
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 60
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 61
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 62
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 63
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 64
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 65
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 66
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 67
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 68
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 69
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 70
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 71
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 72
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 73
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 74
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 75
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 76
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 77
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 78
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 79
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 80
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 81
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 82
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 83
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 84
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 85
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 86
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 87
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 88
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 89
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 90
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 91
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 92
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 93
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 94
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 95
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 96
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 97
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 98
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 99
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">100
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">101
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">102
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">103
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">104
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">105
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">106
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">107
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">108
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">109
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">110
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">111
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">112
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">113
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">114
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">115
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">116
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">117
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">118
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">119
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">120
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">121
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">122
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">123
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">124
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">125
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#d75f00>from</span> typing <span style=color:#d75f00>import</span> Tuple
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> torch
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> robustness <span style=color:#d75f00>import</span> attack_steps
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> robustness.attacker <span style=color:#d75f00>import</span> AttackerModel
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> torch <span style=color:#d75f00>import</span> Tensor
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> tqdm <span style=color:#d75f00>import</span> tqdm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>class</span> <span style=color:#0087ff>L2MomentumStep</span>(attack_steps.AttackerStep):
</span></span><span style=display:flex><span>    <span style=color:#00afaf>&#34;&#34;&#34;L2 Momentum for faster convergence of inversion process&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> __init__(
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>,
</span></span><span style=display:flex><span>        orig_input: Tensor,
</span></span><span style=display:flex><span>        eps: <span style=color:#0087ff>float</span>,
</span></span><span style=display:flex><span>        step_size: <span style=color:#0087ff>float</span>,
</span></span><span style=display:flex><span>        use_grad: <span style=color:#0087ff>bool</span> = <span style=color:#d75f00>True</span>,
</span></span><span style=display:flex><span>        momentum: <span style=color:#0087ff>float</span> = <span style=color:#00afaf>0.9</span>,
</span></span><span style=display:flex><span>        ):
</span></span><span style=display:flex><span>        <span style=color:#0087ff>super</span>().__init__(orig_input, eps, step_size, use_grad=use_grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.momentum_g = torch.zeros_like(orig_input)
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.gamma = momentum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> <span style=color:#0087ff>project</span>(<span style=color:#0087ff>self</span>, x: Tensor) -&gt; Tensor:
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;&#34;&#34;Ensures inversion does not go outside of `self.eps` L2 ball&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        diff = x - <span style=color:#0087ff>self</span>.orig_input
</span></span><span style=display:flex><span>        diff = diff.renorm(p=<span style=color:#00afaf>2</span>, dim=<span style=color:#00afaf>0</span>, maxnorm=<span style=color:#0087ff>self</span>.eps)
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> torch.clamp(<span style=color:#0087ff>self</span>.orig_input + diff, <span style=color:#00afaf>0</span>, <span style=color:#00afaf>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> <span style=color:#0087ff>step</span>(<span style=color:#0087ff>self</span>, x: Tensor, g: Tensor) -&gt; Tensor:
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;&#34;&#34;Steps along gradient with L2 momentum&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        g = g / g.norm(dim=(<span style=color:#00afaf>1</span>, <span style=color:#00afaf>2</span>, <span style=color:#00afaf>3</span>), p=<span style=color:#00afaf>2</span>, keepdim=<span style=color:#d75f00>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.momentum_g = <span style=color:#0087ff>self</span>.momentum_g * <span style=color:#0087ff>self</span>.gamma + g * (<span style=color:#00afaf>1.0</span> - <span style=color:#0087ff>self</span>.gamma)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> x + <span style=color:#0087ff>self</span>.momentum_g * <span style=color:#0087ff>self</span>.step_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>inversion_loss</span>(
</span></span><span style=display:flex><span>    model: AttackerModel, inp: Tensor, targ: Tensor
</span></span><span style=display:flex><span>) -&gt; Tuple[Tensor, <span style=color:#d75f00>None</span>]:
</span></span><span style=display:flex><span>    <span style=color:#00afaf>&#34;&#34;&#34;L2 distance between target representation and current inversion representation&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    _, rep = model(inp, with_latent=<span style=color:#d75f00>True</span>, fake_relu=<span style=color:#d75f00>False</span>)
</span></span><span style=display:flex><span>    loss = torch.div(torch.norm(rep - targ, dim=<span style=color:#00afaf>1</span>), torch.norm(targ, dim=<span style=color:#00afaf>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> loss, <span style=color:#d75f00>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>invert_images</span>(
</span></span><span style=display:flex><span>    model: AttackerModel,
</span></span><span style=display:flex><span>    seed_images: Tensor,
</span></span><span style=display:flex><span>    target_images: Tensor,
</span></span><span style=display:flex><span>    batch_size: <span style=color:#0087ff>int</span> = <span style=color:#00afaf>32</span>,
</span></span><span style=display:flex><span>    step_size: <span style=color:#0087ff>float</span> = <span style=color:#00afaf>1.0</span> / <span style=color:#00afaf>8.0</span>,
</span></span><span style=display:flex><span>    iterations: <span style=color:#0087ff>int</span> = <span style=color:#00afaf>2_000</span>,
</span></span><span style=display:flex><span>    use_best: <span style=color:#0087ff>bool</span> = <span style=color:#d75f00>True</span>,
</span></span><span style=display:flex><span>) -&gt; Tensor:
</span></span><span style=display:flex><span>    <span style=color:#00afaf>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    Representation inversion process as described in
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `If You&#39;ve Trained One You&#39;ve Trained Them All: Inter-Architecture Similarity Increases With Robustness`
</span></span></span><span style=display:flex><span><span style=color:#00afaf>
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    Default hyperparameters are exactly as used in paper.
</span></span></span><span style=display:flex><span><span style=color:#00afaf>
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    Parameters
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    ----------
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `model` : AttackerModel
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Model to invert through, should be a robustness.attacker.AttackerModel
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `seed_images` : Tensor
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Tensor of seed images, [B, C, H, W]
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `target_images` : Tensor
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Tensor of corresponding target images [B, C, H, W]
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `batch_size` : int, optional
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Number of images to invert at once
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `step_size` : float, optional
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    &#39;learning rate&#39; of backprop step
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `iterations` : int, optional
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Number of back prop iterations
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    `use_best` : bool
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Use best inversion found rather than last
</span></span></span><span style=display:flex><span><span style=color:#00afaf>
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    Returns
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    -------
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    Tensor
</span></span></span><span style=display:flex><span><span style=color:#00afaf>        Resulting inverted images [B, C, H, W]
</span></span></span><span style=display:flex><span><span style=color:#00afaf>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># L2 Momentum step</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> <span style=color:#0087ff>constraint</span>(orig_input, eps, step_size):
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> L2MomentumStep(orig_input, eps, step_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Arguments for inversion</span>
</span></span><span style=display:flex><span>    kwargs = {
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;constraint&#34;</span>:  constraint,
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;step_size&#34;</span>:   step_size,
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;iterations&#34;</span>:  iterations,
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;eps&#34;</span>:         <span style=color:#00afaf>1000</span>, <span style=color:#4e4e4e># Set to large number as we are not constraining inversion</span>
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;custom_loss&#34;</span>: inversion_loss,
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;targeted&#34;</span>:    <span style=color:#d75f00>True</span>, <span style=color:#4e4e4e># Minimize loss</span>
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;use_best&#34;</span>:    use_best,
</span></span><span style=display:flex><span>        <span style=color:#00afaf>&#34;do_tqdm&#34;</span>:     <span style=color:#d75f00>False</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Batch input</span>
</span></span><span style=display:flex><span>    seed_batches   = seed_images.split(batch_size)
</span></span><span style=display:flex><span>    target_batches = target_images.split(batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Begin inversion process</span>
</span></span><span style=display:flex><span>    inverted = []
</span></span><span style=display:flex><span>    <span style=color:#5f8700>for</span> init_imgs, targ_imgs <span style=color:#5f8700>in</span> tqdm(
</span></span><span style=display:flex><span>        <span style=color:#0087ff>zip</span>(seed_batches, target_batches),
</span></span><span style=display:flex><span>        total=<span style=color:#0087ff>len</span>(seed_batches),
</span></span><span style=display:flex><span>        leave=<span style=color:#d75f00>True</span>,
</span></span><span style=display:flex><span>        desc=<span style=color:#00afaf>&#34;Inverting&#34;</span>,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># Get activations from target images</span>
</span></span><span style=display:flex><span>        (_, rep_targ), _ = model(targ_imgs.cuda(), with_latent=<span style=color:#d75f00>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># Minimize distance from seed representation to target representation</span>
</span></span><span style=display:flex><span>        (_, _), inv = model(
</span></span><span style=display:flex><span>            init_imgs.cuda(), rep_targ, make_adv=<span style=color:#d75f00>True</span>, with_latent=<span style=color:#d75f00>True</span>, **kwargs
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        inverted.append(inv.detach().cpu())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    inverted = torch.vstack(inverted)
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> inverted</span></span></code></pre></td></tr></table></div></div><h1 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h1><p>While I mainly focused on our proposed metric in this article, I briefly wanted to discuss some of the interesting takeaways we included in the paper. The fact that robustness systematically increases a model’s similarity to any arbitrary other model, regardless of architecture or initialization, is a significant one. Within the representation learning community, some researchers have posed a <strong>universality hypothesis</strong>. This hypothesis conjectures that the features networks learn from their data are <strong>universal</strong>‚ in that they are shared across distinct initializations or architectures. Our results imply a <em>modified</em> universality hypothesis, suggesting that under sufficient constraints (i.e., a robustness constraint), diverse architectures will converge on a similar set of learned features. This could mean that empirical analysis of a <em>single</em> robust neural network can reveal insight into <em>every</em> other neural network&ndash;possibly bringing us closer to understanding the nature of adversarial robustness itself. This is especially exciting in light of research looking at similarities between the representations learned by neural networks and brains.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] Haydn T. Jones, Jacob M. Springer, Garrett T. Kenyon, and Juston S. Moore. <a href="https://openreview.net/forum?id=BGfLS_8j5eq">&ldquo;If You&rsquo;ve Trained One You&rsquo;ve Trained Them All: Inter-Architecture Similarity Increases With Robustness.&rdquo;</a> UAI (2022).</p><p>[2] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. <a href=https://arxiv.org/abs/1905.00414>&ldquo;Similarity of Neural Network Representations Revisited.&rdquo;</a> ICML (2019).</p><p>[3] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. <a href=https://arxiv.org/abs/1905.02175>“Adversarial Examples Are Not Bugs, They are Features.”</a> NeurIPS (2019).</p><p>[4] Vedant Nanda, Till Speicher, Camila Kolling, John P. Dickerson, Krishna Gummadi, and Adrian Weller. <a href=https://arxiv.org/abs/2206.11939>&ldquo;Measuring Representational Robustness of Neural Networks Through Shared Invariances.”</a> ICML (2022).</p><hr><p>LA-UR-22-27916</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>We define the representation layer to be the layer before the fully connected output layer. We are most interested in this layer as it should have the richest representation of the input image.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>It turns out that our metric of network similarity was simultaneously proposed and published by Nanda et al. in <a href=https://arxiv.org/abs/2206.11939>&ldquo;Measuring Representational Robustness of Neural Networks Through Shared Invariances”</a>, where they call this metric “STIR”.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://haydn.fgl.dev/tags/ml/>ML</a></li></ul><nav class=paginav><a class=next href=https://haydn.fgl.dev/posts/the-launch-of-waifuxl/><span class=title>Next »</span><br><span>The Launch of WaifuXL</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on twitter" href="https://twitter.com/intent/tweet/?text=A%20Better%20Metric%20of%20Neural%20Network%20Similarity&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f&hashtags=ML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f&title=A%20Better%20Metric%20of%20Neural%20Network%20Similarity&summary=A%20Better%20Metric%20of%20Neural%20Network%20Similarity&source=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f&title=A%20Better%20Metric%20of%20Neural%20Network%20Similarity"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on whatsapp" href="https://api.whatsapp.com/send?text=A%20Better%20Metric%20of%20Neural%20Network%20Similarity%20-%20https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A Better Metric of Neural Network Similarity on telegram" href="https://telegram.me/share/url?text=A%20Better%20Metric%20of%20Neural%20Network%20Similarity&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fa-better-index-of-similarity%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://haydn.fgl.dev/>Posterior Collapse</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>