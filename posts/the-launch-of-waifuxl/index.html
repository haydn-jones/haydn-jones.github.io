<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Launch of WaifuXL | Posterior Collapse</title><meta name=keywords content="ML"><meta name=description content="We&rsquo;re Launching at waifuxl.com! Today we&rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we&rsquo;re really excited to share it with you.
WaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different."><meta name=author content="Haydn Jones, Alec Benson, Benjamin Mastripolito, The Future Gadgets Lab"><link rel=canonical href=https://haydn.fgl.dev/posts/the-launch-of-waifuxl/><link crossorigin=anonymous href=/assets/css/stylesheet.646ae74ba632bf26de0bdd1659630c1cfd619f05bccbdd4f05c9afbd3dad4560.css integrity="sha256-ZGrnS6YyvybeC90WWWMMHP1hnwW8y91PBcmvvT2tRWA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://haydn.fgl.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://haydn.fgl.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://haydn.fgl.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://haydn.fgl.dev/apple-touch-icon.png><link rel=mask-icon href=https://haydn.fgl.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/gruvbox-dark.min.css integrity="sha512-qRo8LjYAm+c8yc6YH84TROIPnkx6j0bD1m1GbA/97FemUlk5y0bYyDbXBWLlWMEBT/AaTUofS4KS3stTdW2RFg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity=sha384-ZPe7yZ91iWxYumsBEOn7ieg8q/o+qh/hQpSaPow8T6BwALcXSCS6C6fSRPIAnTQs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity=sha384-ljao5I1l+8KYFXG7LNEA7DyaFvuvSCmedUf6Y6JI7LJqiu8q5dEivP2nDdFH31V4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','')</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-V58YRV80SX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-V58YRV80SX',{anonymize_ip:!1})}</script><meta property="og:title" content="The Launch of WaifuXL"><meta property="og:description" content="We&rsquo;re Launching at waifuxl.com! Today we&rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we&rsquo;re really excited to share it with you.
WaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different."><meta property="og:type" content="article"><meta property="og:url" content="https://haydn.fgl.dev/posts/the-launch-of-waifuxl/"><meta property="og:image" content="https://haydn.fgl.dev/posts/the-launch-of-waifuxl/images/WaifuXL.webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-22T14:29:11-06:00"><meta property="article:modified_time" content="2022-05-28T11:56:04-06:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://haydn.fgl.dev/posts/the-launch-of-waifuxl/images/WaifuXL.webp"><meta name=twitter:title content="The Launch of WaifuXL"><meta name=twitter:description content="We&rsquo;re Launching at waifuxl.com! Today we&rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we&rsquo;re really excited to share it with you.
WaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://haydn.fgl.dev/posts/"},{"@type":"ListItem","position":2,"name":"The Launch of WaifuXL","item":"https://haydn.fgl.dev/posts/the-launch-of-waifuxl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Launch of WaifuXL","name":"The Launch of WaifuXL","description":"We\u0026rsquo;re Launching at waifuxl.com! Today we\u0026rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we\u0026rsquo;re really excited to share it with you.\nWaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different.","keywords":["ML"],"articleBody":"We’re Launching at waifuxl.com! Today we’re finally launching our neural network powered super resolution website for anime-style drawings, WaifuXL! This is a project that The Future Gadgets Lab has been working on for a while and we’re really excited to share it with you.\nWaifuXL is quite similar to waifu2x in function, however, our super resolution model (the Real-ESRGAN) produces much better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different. When you use our service to upscale an image, rather than sending your input to a backend somewhere in the cloud to be up-sampled remotely, we send the up-sampling neural network (and the tagger) to you for execution directly on your laptop, desktop, phone, or tablet. We’ll get to how this is possible in a moment, but first we’re going to cover the models.\nThe Networks Super Resolution What sets the Real-ESRGAN apart from other models (and the models used by waifu2x) is not the architecture but it’s training process. The standard training process for super resolution networks is to simply take a high-resolution dataset of images, downscale them to a lower resolution, and train the network to map from the downscaled images to the original high-resolution ones. The Real-ESRGAN training process attempts to directly model the kind of degradations one might encounter in real-world low-quality images through a process they call a “high-order degradation model”. During this process they combine many different degradations with various intensities to produce a visually corrupt input to the network. Here is an overview of the various degradations they apply, but refer to Section 3 of the paper to get a better idea of the entire process:\nBlur (multiple kinds) Noise Combinations of up-sampling and down-sampling with various algorithms JPEG compression Sinc filters (to model ringing and overshoot) The training process is further improved by including a GAN loss (wherein a separate network learns to differentiate between the true high-res images and the super resolution network outputs, causing the two networks to compete) and a Perceptual loss.\nImage Tagging For our image tagger we’re using a MobileNetV3. The network will detect 2,000 different characteristics in the image, 2,000 different characters (mostly from anime or related media), and an explicitness rating (safe, questionable, or explicit). Here is an example of our tagger outputs:\nCharacteristics Number of girls / boys in the image Eye color Hair color Clothing types Characters: Naruto Hatsune Miku Megumin The Dataset To train both of our models we are using the Danbooru2021 dataset, comprised of ~4.9m images scraped from the Danbooru image board. Due to storage limitations we trained both models on a subset of ~1.2m images, however, this is likely a sizable enough subset to not have caused any problems. The tags in the Danbooru2021 dataset are quite diverse, with over 493,000 distinct tags, but they are extremely unbalanced. The most common tag is “1girl” which is present on millions of the images and by the time you get down to the 1,000th most common tag it is only found on ~10,000 images. The unbalanced nature of the tags certainly has resulted in a less than optimal tagger, but there is little that can be done about that.\nThe Website The Not-Backend Previously we mentioned that rather than running the models on a backend in the cloud, we’re serving you the models for execution on your machine–how is this done? The Onnx Runtime is an open source project aiming to accelerate machine learning training across a wide variety of frameworks, devices, and deployment targets–one of these targets being the web through a WebAssembly backend. After exporting the model to the onnx format we simply serve the model to you and their backend will take care of the rest. Best of all it supports SIMD acceleration and multithreading.\nThe Upscaling Lifecycle Generally, the lifecycle of WaifuXL can be described as taking in an input image, loading in the model, running the model on the image, and then creating an output image from the model results. There are more granular steps in this process that provide various tools and functionality, such as image chunking and GIF de/restructuring.\nIn order to efficiently pass data between different services and UI components, all of the images are represented throughout the lifecycle as Data URIs. If you’re not familiar, data URIs are a long string that basically provides a text encoding of a file, so they allow for you to store the image purely as a string. They begin with a small section describing the file format (data:image/gif or data:image/png), and then a long string that encodes the actual file contents.\nWhen the actual pixel information is displayed to a user on the site, these data URIs are simply passed into JSX image tags and displayed. When the various services need to actually gather information from the images (such as for chunking, or to build an ndarray to pass to the model) the data URIs are painted on an HTML canvas (this canvas is never shown to the user) and the getImageData function is called on the canvas context.\nThe actual function we use to do this looks as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function getPixelDataFromURI(inputURI) { return new Promise((resolve, reject) =\u003e { const img = new Image(); img.src = inputURI; img.crossOrigin = \"Anonymous\"; var results = null; img.onload = function () { const canvas = document.createElement(\"canvas\"); canvas.width = img.width; canvas.height = img.height; const context = canvas.getContext(\"2d\"); context.drawImage(img, 0, 0); resolve(context.getImageData(0, 0, img.width, img.height)); }; }); } After we have all the pixel information in the image, we then convert this to an ndarray, using the ndarray and ndarray-ops libraries.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 export function buildNdarrayFromImage(imageData) { const { data, width, height } = imageData; const dataTensor = ndarray(new Uint8Array(data), [height, width, 4]); const dataProcessedTensor = ndarray(new Uint8Array(width * height * 3), [ 1, 3, height, width, ]); ops.assign( dataProcessedTensor.pick(0, 0, null, null), dataTensor.pick(null, null, 0) ); ops.assign( dataProcessedTensor.pick(0, 1, null, null), dataTensor.pick(null, null, 1) ); ops.assign( dataProcessedTensor.pick(0, 2, null, null), dataTensor.pick(null, null, 2) ); return dataProcessedTensor; } Finally, this ndarray is converted to a Tensor for the model input, as follows:\n1 2 3 4 5 6 7 8 9 10 11 function prepareImage(imageArray) { const height = imageArray.shape[2]; const width = imageArray.shape[3]; const tensor = new ort.Tensor(\"uint8\", imageArray.data, [ 1, 3, height, width, ]); return { input: tensor }; } The model then runs, using this new Tensor as input, and produces a new Tensor. This Tensor is converted to an ndarray, and we paint this ndarray to a canvas once again, and return the Data URI from this canvas.\n1 2 3 4 5 6 7 8 9 10 export function buildImageFromND(nd, height, width) { const canvas = document.createElement(\"canvas\"); canvas.width = width; canvas.height = height; const context = canvas.getContext(\"2d\"); var data = context.createImageData(width, height); data.data.set(nd); context.putImageData(data, 0, 0); return canvas.toDataURL(); } Finally, now that we have the output Data URI, we can render this output alongside the input to the user, using the react-compare-slider library to create a side-by-side view of the images.\nBreakdown of Some “Fun” Problems Image Splitting After some initial testing with upscaling large images, we realized that with a sufficiently large image, the upscaling model would fail. The cause of this turned out to be the 32-bit address space of WebAssembly, meaning our address space, and therefore our memory usage, is fundamentally limited. To get around this problem, we split large images into chunks and process the individual chunks instead of processing the entire image at once.\nThat would’ve been the end of the story if it weren’t for the windowing artifacts. When image chunks are upscaled, there are visible artifacts along their boundaries in the combined image, shown below:\nTo get around this we allow chunks to overlap onto other chunks when they are upscaled, and then remove/merge the overlapping regions. We found that an overlap of only 4 pixels to each side was sufficient to remove the windowing effect.\nGIFs GIFs work almost the exact same way that images do, except for on input they are split into each frame, each of these frames is upscaled individually, and then the frames are reassembled into a new GIF. This is basically just running the image upscaling process on each frame, and then combining these frames into a new GIF. This is accomplished with the help of gif-js, gifuct-js, and gif-extract-frames.\nThe most challenging parts of this process was handling coalescing, since GIFs often only store information about pixels changed between frames rather than each frame of the GIF. Generally, gif-extract-frames is used to break the image down into individual frames, gif.js is used to create new GIFs, and gifuct-js primarily handles extracting the delay from GIFs.\nSafari Non-Compliance One of the more interesting problems arose from a classic Web Development hurdle: non-compliant browsers. And of course, as Internet Explorer has been sunset, a new champion of non-compliance rises – Apple’s Safari browser. While significantly better than previously non-compliant browsers, Safari still offers various “fun” things it’s decided not to implement. In this project, our main issues came from (what we believe) is Safari’s SharedArrayBuffer implementation – namely that it doesn’t seem to work. The ONNX web runtime uses SharedArrayBuffers when running multi-threaded, so in Safari, trying to initialize the model to use more than one thread fails. At this time, we’re getting around this by checking the User Agent of the browser, and if its a Webkit based browser / engine we fall back to serial execution. We’ve submitted an issue with ONNX to resolve this, and hopefully we will be able to give Webkit based browser users better speeds in the future.\nAs a sidenote, to enable SharedArrayBuffers in general you must set two response headers–Cross Origin Embedder Policy and Cross Origin Opener Policy. When you don’t set these headers properly, there will be no issue thrown on any browser, as it is impossible for SharedArrayBuffers to be used at all. This led to plenty of confusion in local testing and early trials, as it became difficult to efficiently test changes and debug the issue locally.\nDisagreement between PyTorch and ONNX One operation used by the Real-ESRGAN is the Pixel UnShuffle, an operation where spatial width and height of an image (or Tensor) are traded for depth by squeezing some of the spatial information into the channel dimension. Both PyTorch and ONNX support this operation, however, they were performing this squeezing in different orders. This was resulting in an upscaled image that looks like the colors were inverted–not great. An issue was opened in PyTorch and in the meantime we implemented the operator from scratch. About a week ago the issue was finally resolved, and we were able to use the built in version.\nWrap-up and Future Work for the Future Gadgets Lab It was a bit all over the place and rushed, but we hope you enjoyed our write up on WaifuXL. This was our first big project as a group, so we were excited to share some details on our effort.\nWe hope you like WaifuXL, we’re really happy with the quality of the model (of course, props to the researchers behind the Real-ESRGAN) and our method of delivery. Our model only performs well on anime-style drawings but we’d like to train a model on real images so we can provide high quality up-sampling for all types of images. We’re also interested in adding some more models to the website, such as a style transfer model, but we’re likely going to leave that to a later date.\nStay tuned for some more fun projects from us, we’re always throwing around ideas and maybe we’ll land on another one like this soon. Until then, keep expanding your Waifus.\n","wordCount":"1993","inLanguage":"en","image":"https://haydn.fgl.dev/posts/the-launch-of-waifuxl/images/WaifuXL.webp","datePublished":"2022-05-22T14:29:11-06:00","dateModified":"2022-05-28T11:56:04-06:00","author":[{"@type":"Person","name":"Haydn Jones"},{"@type":"Person","name":"Alec Benson"},{"@type":"Person","name":"Benjamin Mastripolito"},{"@type":"Person","name":"The Future Gadgets Lab"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://haydn.fgl.dev/posts/the-launch-of-waifuxl/"},"publisher":{"@type":"Organization","name":"Posterior Collapse","logo":{"@type":"ImageObject","url":"https://haydn.fgl.dev/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://haydn.fgl.dev/ accesskey=h title="Posterior Collapse (Alt + H)">Posterior Collapse</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://haydn.fgl.dev/archives title=Archive><span>Archive</span></a></li><li><a href=https://haydn.fgl.dev/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://haydn.fgl.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://haydn.fgl.dev/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://waifuxl.com/ title=WaifuXL.com><span>WaifuXL.com</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://haydn.fgl.dev/>Home</a>&nbsp;»&nbsp;<a href=https://haydn.fgl.dev/posts/>Posts</a></div><h1 class=post-title>The Launch of WaifuXL</h1><div class=post-meta><span title='2022-05-22 14:29:11 -0600 -0600'>May 22, 2022</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Haydn Jones, Alec Benson, Benjamin Mastripolito, The Future Gadgets Lab&nbsp;|&nbsp;<a href=https://github.com/haydn-jones/haydn-jones.github.io/tree/main/content//posts/The-Launch-of-WaifuXL/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src=https://haydn.fgl.dev/posts/the-launch-of-waifuxl/images/WaifuXL.webp alt="WaifuXL Screenshot"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-networks>The Networks</a><ul><li><a href=#super-resolution>Super Resolution</a></li><li><a href=#image-tagging>Image Tagging</a></li><li><a href=#the-dataset>The Dataset</a></li></ul></li><li><a href=#the-website>The Website</a><ul><li><a href=#the-not-backend>The Not-Backend</a></li><li><a href=#the-upscaling-lifecycle>The Upscaling Lifecycle</a></li></ul></li><li><a href=#breakdown-of-some-fun-problems>Breakdown of Some &ldquo;Fun&rdquo; Problems</a><ul><li><a href=#image-splitting>Image Splitting</a></li><li><a href=#gifs>GIFs</a></li><li><a href=#safari-non-compliance>Safari Non-Compliance</a></li><li><a href=#disagreement-between-pytorch-and-onnx>Disagreement between PyTorch and ONNX</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=were-launching-at-waifuxlcomhttpswaifuxlcom>We&rsquo;re Launching at <a href=https://waifuxl.com/>waifuxl.com</a>!<a hidden class=anchor aria-hidden=true href=#were-launching-at-waifuxlcomhttpswaifuxlcom>#</a></h1><p>Today we&rsquo;re finally launching our neural network powered super resolution website for anime-style drawings, <a href=https://waifuxl.com/>WaifuXL</a>! This is a project that <a href=https://github.com/TheFutureGadgetsLab/WaifuXL>The Future Gadgets Lab</a> has been working on for a while and we&rsquo;re really excited to share it with you.</p><p>WaifuXL is quite similar to <a href=http://waifu2x.udp.jp/>waifu2x</a> in function, however, our super resolution model (the <a href=https://arxiv.org/abs/2107.10833>Real-ESRGAN</a>) produces <em><strong>much</strong></em> better up-samples, we have a fun image property tagger, and our backend (or lack thereof) is radically different. When you use our service to upscale an image, rather than sending your input to a backend somewhere in the cloud to be up-sampled remotely, we send the up-sampling neural network (and the tagger) <em>to you</em> for execution directly on your laptop, desktop, phone, or tablet. We&rsquo;ll get to how this is possible in a moment, but first we&rsquo;re going to cover the models.</p><p><img loading=lazy src=images/comparison.webp alt=ComparisonWithWaifu2x></p><h2 id=the-networks>The Networks<a hidden class=anchor aria-hidden=true href=#the-networks>#</a></h2><h3 id=super-resolution>Super Resolution<a hidden class=anchor aria-hidden=true href=#super-resolution>#</a></h3><p>What sets the Real-ESRGAN apart from other models (and the models used by waifu2x) is not the architecture but it’s <em>training process</em>. The standard training process for super resolution networks is to simply take a high-resolution dataset of images, downscale them to a lower resolution, and train the network to map from the downscaled images to the original high-resolution ones. The Real-ESRGAN training process attempts to directly model the kind of degradations one might encounter in real-world low-quality images through a process they call a &ldquo;high-order degradation model&rdquo;. During this process they combine many different degradations with various intensities to produce a visually corrupt input to the network. Here is an overview of the various degradations they apply, but refer to <strong>Section 3</strong> of the paper to get a better idea of the entire process:</p><ul><li>Blur (multiple kinds)</li><li>Noise</li><li>Combinations of up-sampling and down-sampling with various algorithms</li><li>JPEG compression</li><li>Sinc filters (to model ringing and overshoot)</li></ul><p>The training process is further improved by including a GAN loss (wherein a separate network learns to differentiate between the true high-res images and the super resolution network outputs, causing the two networks to compete) and a <a href=https://arxiv.org/abs/1801.03924>Perceptual</a> loss.</p><h3 id=image-tagging>Image Tagging<a hidden class=anchor aria-hidden=true href=#image-tagging>#</a></h3><p>For our image tagger we&rsquo;re using a <a href=https://arxiv.org/abs/1905.02244>MobileNetV3</a>. The network will detect 2,000 different characteristics in the image, 2,000 different characters (mostly from anime or related media), and an explicitness rating (safe, questionable, or explicit). Here is an example of our tagger outputs:</p><ul><li>Characteristics<ul><li>Number of girls / boys in the image</li><li>Eye color</li><li>Hair color</li><li>Clothing types</li></ul></li><li>Characters:<ul><li>Naruto</li><li>Hatsune Miku</li><li>Megumin</li></ul></li></ul><h3 id=the-dataset>The Dataset<a hidden class=anchor aria-hidden=true href=#the-dataset>#</a></h3><p>To train both of our models we are using the <a href=https://www.gwern.net/Danbooru2021>Danbooru2021</a> dataset, comprised of ~4.9m images scraped from the Danbooru image board. Due to storage limitations we trained both models on a subset of ~1.2m images, however, this is likely a sizable enough subset to not have caused any problems. The tags in the Danbooru2021 dataset are quite diverse, with over 493,000 distinct tags, but they are <em>extremely</em> unbalanced. The most common tag is &ldquo;1girl&rdquo; which is present on millions of the images and by the time you get down to the 1,000th most common tag it is only found on ~10,000 images. The unbalanced nature of the tags certainly has resulted in a less than optimal tagger, but there is little that can be done about that.</p><h2 id=the-website>The Website<a hidden class=anchor aria-hidden=true href=#the-website>#</a></h2><h3 id=the-not-backend>The Not-Backend<a hidden class=anchor aria-hidden=true href=#the-not-backend>#</a></h3><p>Previously we mentioned that rather than running the models on a backend in the cloud, we&rsquo;re serving you the models for execution on your machine&ndash;how is this done? The <a href=https://onnxruntime.ai/>Onnx Runtime</a> is an open source project aiming to accelerate machine learning training across a wide variety of frameworks, devices, and deployment targets&ndash;one of these targets being the web through a WebAssembly backend. After exporting the model to the <code>onnx</code> format we simply serve the model to you and their backend will take care of the rest. Best of all it supports SIMD acceleration and multithreading.</p><h3 id=the-upscaling-lifecycle>The Upscaling Lifecycle<a hidden class=anchor aria-hidden=true href=#the-upscaling-lifecycle>#</a></h3><p>Generally, the lifecycle of WaifuXL can be described as taking in an input image, loading in the model, running the model on the image, and then creating an output image from the model results. There are more granular steps in this process that provide various tools and functionality, such as image chunking and GIF de/restructuring.</p><p>In order to efficiently pass data between different services and UI components, all of the images are represented throughout the lifecycle as Data URIs. If you&rsquo;re not familiar, data URIs are a long string that basically provides a text encoding of a file, so they allow for you to store the image purely as a string. They begin with a small section describing the file format (<code>data:image/gif</code> or <code>data:image/png</code>), and then a long string that encodes the actual file contents.</p><p>When the actual pixel information is displayed to a user on the site, these data URIs are simply passed into JSX image tags and displayed. When the various services need to actually gather information from the images (such as for chunking, or to build an ndarray to pass to the model) the data URIs are painted on an HTML canvas (this canvas is never shown to the user) and the <code>getImageData</code> function is called on the canvas context.<br>The actual function we use to do this looks as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Javascript data-lang=Javascript><span class=line><span class=cl><span class=kd>function</span> <span class=nx>getPixelDataFromURI</span><span class=p>(</span><span class=nx>inputURI</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=k>new</span> <span class=nb>Promise</span><span class=p>((</span><span class=nx>resolve</span><span class=p>,</span> <span class=nx>reject</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>img</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>Image</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=nx>img</span><span class=p>.</span><span class=nx>src</span> <span class=o>=</span> <span class=nx>inputURI</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nx>img</span><span class=p>.</span><span class=nx>crossOrigin</span> <span class=o>=</span> <span class=s2>&#34;Anonymous&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kd>var</span> <span class=nx>results</span> <span class=o>=</span> <span class=kc>null</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nx>img</span><span class=p>.</span><span class=nx>onload</span> <span class=o>=</span> <span class=kd>function</span> <span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kr>const</span> <span class=nx>canvas</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>createElement</span><span class=p>(</span><span class=s2>&#34;canvas&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=nx>canvas</span><span class=p>.</span><span class=nx>width</span> <span class=o>=</span> <span class=nx>img</span><span class=p>.</span><span class=nx>width</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>canvas</span><span class=p>.</span><span class=nx>height</span> <span class=o>=</span> <span class=nx>img</span><span class=p>.</span><span class=nx>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=kr>const</span> <span class=nx>context</span> <span class=o>=</span> <span class=nx>canvas</span><span class=p>.</span><span class=nx>getContext</span><span class=p>(</span><span class=s2>&#34;2d&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=nx>context</span><span class=p>.</span><span class=nx>drawImage</span><span class=p>(</span><span class=nx>img</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=nx>resolve</span><span class=p>(</span><span class=nx>context</span><span class=p>.</span><span class=nx>getImageData</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=nx>img</span><span class=p>.</span><span class=nx>width</span><span class=p>,</span> <span class=nx>img</span><span class=p>.</span><span class=nx>height</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>};</span>
</span></span><span class=line><span class=cl>  <span class=p>});</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>After we have all the pixel information in the image, we then convert this to an <code>ndarray</code>, using the <code>ndarray</code> and <code>ndarray-ops</code> libraries.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Javascript data-lang=Javascript><span class=line><span class=cl><span class=kr>export</span> <span class=kd>function</span> <span class=nx>buildNdarrayFromImage</span><span class=p>(</span><span class=nx>imageData</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=p>{</span> <span class=nx>data</span><span class=p>,</span> <span class=nx>width</span><span class=p>,</span> <span class=nx>height</span> <span class=p>}</span> <span class=o>=</span> <span class=nx>imageData</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>dataTensor</span> <span class=o>=</span> <span class=nx>ndarray</span><span class=p>(</span><span class=k>new</span> <span class=nx>Uint8Array</span><span class=p>(</span><span class=nx>data</span><span class=p>),</span> <span class=p>[</span><span class=nx>height</span><span class=p>,</span> <span class=nx>width</span><span class=p>,</span> <span class=mi>4</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>dataProcessedTensor</span> <span class=o>=</span> <span class=nx>ndarray</span><span class=p>(</span><span class=k>new</span> <span class=nx>Uint8Array</span><span class=p>(</span><span class=nx>width</span> <span class=o>*</span> <span class=nx>height</span> <span class=o>*</span> <span class=mi>3</span><span class=p>),</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>height</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>width</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>]);</span>
</span></span><span class=line><span class=cl>  <span class=nx>ops</span><span class=p>.</span><span class=nx>assign</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataProcessedTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nx>ops</span><span class=p>.</span><span class=nx>assign</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataProcessedTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nx>ops</span><span class=p>.</span><span class=nx>assign</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataProcessedTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=nx>dataTensor</span><span class=p>.</span><span class=nx>pick</span><span class=p>(</span><span class=kc>null</span><span class=p>,</span> <span class=kc>null</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>dataProcessedTensor</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Finally, this <code>ndarray</code> is converted to a <code>Tensor</code> for the model input, as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Javascript data-lang=Javascript><span class=line><span class=cl><span class=kd>function</span> <span class=nx>prepareImage</span><span class=p>(</span><span class=nx>imageArray</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>height</span> <span class=o>=</span> <span class=nx>imageArray</span><span class=p>.</span><span class=nx>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>width</span> <span class=o>=</span> <span class=nx>imageArray</span><span class=p>.</span><span class=nx>shape</span><span class=p>[</span><span class=mi>3</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>tensor</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>ort</span><span class=p>.</span><span class=nx>Tensor</span><span class=p>(</span><span class=s2>&#34;uint8&#34;</span><span class=p>,</span> <span class=nx>imageArray</span><span class=p>.</span><span class=nx>data</span><span class=p>,</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>height</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nx>width</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>]);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=p>{</span> <span class=nx>input</span><span class=o>:</span> <span class=nx>tensor</span> <span class=p>};</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>The model then runs, using this new Tensor as input, and produces a new Tensor. This Tensor is converted to an ndarray, and we paint this ndarray to a canvas once again, and return the Data URI from this canvas.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Javascript data-lang=Javascript><span class=line><span class=cl><span class=kr>export</span> <span class=kd>function</span> <span class=nx>buildImageFromND</span><span class=p>(</span><span class=nx>nd</span><span class=p>,</span> <span class=nx>height</span><span class=p>,</span> <span class=nx>width</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>canvas</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>createElement</span><span class=p>(</span><span class=s2>&#34;canvas&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nx>canvas</span><span class=p>.</span><span class=nx>width</span> <span class=o>=</span> <span class=nx>width</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=nx>canvas</span><span class=p>.</span><span class=nx>height</span> <span class=o>=</span> <span class=nx>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>context</span> <span class=o>=</span> <span class=nx>canvas</span><span class=p>.</span><span class=nx>getContext</span><span class=p>(</span><span class=s2>&#34;2d&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=kd>var</span> <span class=nx>data</span> <span class=o>=</span> <span class=nx>context</span><span class=p>.</span><span class=nx>createImageData</span><span class=p>(</span><span class=nx>width</span><span class=p>,</span> <span class=nx>height</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nx>data</span><span class=p>.</span><span class=nx>data</span><span class=p>.</span><span class=nx>set</span><span class=p>(</span><span class=nx>nd</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nx>context</span><span class=p>.</span><span class=nx>putImageData</span><span class=p>(</span><span class=nx>data</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>canvas</span><span class=p>.</span><span class=nx>toDataURL</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Finally, now that we have the output Data URI, we can render this output alongside the input to the user, using the react-compare-slider library to create a side-by-side view of the images.</p><h2 id=breakdown-of-some-fun-problems>Breakdown of Some &ldquo;Fun&rdquo; Problems<a hidden class=anchor aria-hidden=true href=#breakdown-of-some-fun-problems>#</a></h2><h3 id=image-splitting>Image Splitting<a hidden class=anchor aria-hidden=true href=#image-splitting>#</a></h3><p>After some initial testing with upscaling large images, we realized that with a sufficiently large image, the upscaling model would fail. The cause of this turned out to be the 32-bit address space of WebAssembly, meaning our address space, and therefore our memory usage, is fundamentally limited. To get around this problem, we split large images into chunks and process the individual chunks instead of processing the entire image at once.</p><p>That would&rsquo;ve been the end of the story if it weren&rsquo;t for the windowing artifacts. When image chunks are upscaled, there are visible artifacts along their boundaries in the combined image, shown below:</p><p><img loading=lazy src=images/padding.webp alt=PaddingIssue></p><p>To get around this we allow chunks to <em>overlap</em> onto other chunks when they are upscaled, and then remove/merge the overlapping regions. We found that an overlap of only 4 pixels to each side was sufficient to remove the windowing effect.</p><h3 id=gifs>GIFs<a hidden class=anchor aria-hidden=true href=#gifs>#</a></h3><p>GIFs work almost the exact same way that images do, except for on input they are split into each frame, each of these frames is upscaled individually, and then the frames are reassembled into a new GIF. This is basically just running the image upscaling process on each frame, and then combining these frames into a new GIF. This is accomplished with the help of <a href=https://github.com/jnordberg/gif.js>gif-js</a>, <a href=https://github.com/matt-way/gifuct-js>gifuct-js</a>, and <a href=https://github.com/transitive-bullshit/gif-extract-frames>gif-extract-frames</a>.</p><p>The most challenging parts of this process was handling coalescing, since GIFs often only store information about pixels changed between frames rather than each frame of the GIF. Generally, <code>gif-extract-frames</code> is used to break the image down into individual frames, <code>gif.js</code> is used to create new GIFs, and <code>gifuct-js</code> primarily handles extracting the delay from GIFs.</p><h3 id=safari-non-compliance>Safari Non-Compliance<a hidden class=anchor aria-hidden=true href=#safari-non-compliance>#</a></h3><p>One of the more interesting problems arose from a classic Web Development hurdle: non-compliant browsers. And of course, as Internet Explorer has been sunset, a new champion of non-compliance rises &ndash; Apple&rsquo;s Safari browser. While significantly better than previously non-compliant browsers, Safari still offers various &ldquo;fun&rdquo; things it&rsquo;s decided not to implement. In this project, our main issues came from (what we believe) is Safari&rsquo;s SharedArrayBuffer implementation &ndash; namely that it doesn&rsquo;t seem to work. The ONNX web runtime uses SharedArrayBuffers when running multi-threaded, so in Safari, trying to initialize the model to use more than one thread fails. At this time, we&rsquo;re getting around this by checking the User Agent of the browser, and if its a Webkit based browser / engine we fall back to serial execution. We&rsquo;ve submitted an issue with ONNX to resolve this, and hopefully we will be able to give Webkit based browser users better speeds in the future.</p><p>As a sidenote, to enable SharedArrayBuffers in general you must set two response headers&ndash;Cross Origin Embedder Policy and Cross Origin Opener Policy. When you don&rsquo;t set these headers properly, there will be no issue thrown on any browser, as it is impossible for SharedArrayBuffers to be used at all. This led to plenty of confusion in local testing and early trials, as it became difficult to efficiently test changes and debug the issue locally.</p><h3 id=disagreement-between-pytorch-and-onnx>Disagreement between PyTorch and ONNX<a hidden class=anchor aria-hidden=true href=#disagreement-between-pytorch-and-onnx>#</a></h3><p>One operation used by the Real-ESRGAN is the Pixel UnShuffle, an operation where spatial width and height of an image (or Tensor) are traded for <em>depth</em> by squeezing some of the spatial information into the channel dimension. Both PyTorch and ONNX support this operation, however, they were performing this squeezing in different orders. This was resulting in an upscaled image that looks like the colors were inverted&ndash;not great. An issue was opened in PyTorch and in the meantime we implemented the operator from scratch. About a week ago the issue was finally resolved, and we were able to use the built in version.</p><h1 id=wrap-up-and-future-work-for-the-future-gadgets-lab>Wrap-up and Future Work for the Future Gadgets Lab<a hidden class=anchor aria-hidden=true href=#wrap-up-and-future-work-for-the-future-gadgets-lab>#</a></h1><p>It was a bit all over the place and rushed, but we hope you enjoyed our write up on WaifuXL. This was our first big project as a group, so we were excited to share some details on our effort.</p><p>We hope you like <a href=https://waifuxl.com/>WaifuXL</a>, we&rsquo;re really happy with the quality of the model (of course, props to the researchers behind the Real-ESRGAN) and our method of delivery. Our model only performs well on anime-style drawings but we&rsquo;d like to train a model on real images so we can provide high quality up-sampling for all types of images. We&rsquo;re also interested in adding some more models to the website, such as a style transfer model, but we&rsquo;re likely going to leave that to a later date.</p><p>Stay tuned for some more fun projects from us, we&rsquo;re always throwing around ideas and maybe we&rsquo;ll land on another one like this soon. Until then, keep expanding your Waifus.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://haydn.fgl.dev/tags/ml/>ML</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on twitter" href="https://twitter.com/intent/tweet/?text=The%20Launch%20of%20WaifuXL&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f&hashtags=ML"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f&title=The%20Launch%20of%20WaifuXL&summary=The%20Launch%20of%20WaifuXL&source=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f&title=The%20Launch%20of%20WaifuXL"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on whatsapp" href="https://api.whatsapp.com/send?text=The%20Launch%20of%20WaifuXL%20-%20https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Launch of WaifuXL on telegram" href="https://telegram.me/share/url?text=The%20Launch%20of%20WaifuXL&url=https%3a%2f%2fhaydn.fgl.dev%2fposts%2fthe-launch-of-waifuxl%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://haydn.fgl.dev/>Posterior Collapse</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement('button');e.classList.add('copy-code'),e.innerHTML='copy';function s(){e.innerHTML='copied!',setTimeout(()=>{e.innerHTML='copy'},2e3)}e.addEventListener('click',o=>{if('clipboard'in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand('copy'),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>